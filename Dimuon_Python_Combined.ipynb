{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Physics Analysis: Dimuon spectrum\n",
    "\n",
    "This tutorial shows you how to analyze datasets using `RDataFrame` from a Python notebook. The example analysis performs the following steps:\n",
    "\n",
    "1. Connect a ROOT dataframe to a dataset containing 61 mio. events recorded by CMS in 2012\n",
    "2. Filter the events being relevant for your analysis\n",
    "3. Compute the invariant mass of the selected dimuon candidates\n",
    "4. Plot the invariant mass spectrum showing resonances up to the Z mass\n",
    "\n",
    "This material is based on the analysis done by Stefan Wunsch, available [here](http://opendata.web.cern.ch/record/12342) in CERN's Open Data portal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prelude: Using ROOT from Python\n",
    "\n",
    "While ROOT is implemented in C++, it provides automatic Python bindings, called [PyROOT](https://root.cern/manual/python/). This means that all the ROOT C++ functionality is also available from Python, including `RDataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This import gives access to all the ROOT C++ code from Python\n",
    "import ROOT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelizing an analysis with RDataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An `RDataFrame` analysis written in Python can be executed both *locally* - possibly in parallel on the cores of the machine - and *distributedly* by offloading computations to external resources, including [Spark](https://spark.apache.org/) and [Dask](https://dask.org/) clusters. This feature is enabled by the architecture depicted below, which shows that RDataFrame computation graphs can be mapped to different kinds of resources via backends.\n",
    "\n",
    "In order to parallelize the computation of an analysis, `RDataFrame` divides the input dataset in logical ranges and spawns tasks for each of those ranges on the available resources.\n",
    "\n",
    "<img src=\"DistRDF_architecture.png\" alt=\"Distributed RDataFrame\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a ROOT dataframe\n",
    "\n",
    "The ROOT interface in Python is directly mapped from the C++ API. Since we talk to a C++ library in the back, PyROOT supports to create any C++ type in the Python world out-of-the-box so you can pass data to your C++ library. Python is just the interface language here: all the heavy lifting happens in C++, which is good for performance!\n",
    "\n",
    "Let's see that in action. First we will create a ROOT dataframe that is connected to a dataset named `Events` stored in a ROOT file. The file is pulled in via [XRootD](http://xrootd.org/) from EOS public, but note how it could also be stored in your CERNBox space or in any other EOS repository accessible from SWAN (e.g. the experiment ones).\n",
    "\n",
    "The dataset `Events` is a `TTree` (a \"table\" in first order) and has the following branches (also referred to as \"columns\"):\n",
    "\n",
    "| Branch name | Data type | Description |\n",
    "|-------------|-----------|-------------|\n",
    "| `nMuon` | `unsigned int` | Number of muons in this event |\n",
    "| `Muon_pt` | `float[nMuon]` | Transverse momentum of the muons stored as an array of size `nMuon` |\n",
    "| `Muon_eta` | `float[nMuon]` | Pseudo-rapidity of the muons stored as an array of size `nMuon` |\n",
    "| `Muon_phi` | `float[nMuon]` | Azimuth of the muons stored as an array of size `nMuon` |\n",
    "| `Muon_charge` | `int[nMuon]` | Charge of the muons stored as an array of size `nMuon` and either -1 or 1 |\n",
    "| `Muon_mass` | `float[nMuon]` | Mass of the muons stored as an array of size `nMuon` |\n",
    "\n",
    "The way we create an RDataFrame depends on the target resources where the analysis will run. Below we show how to create an RDataFrame that runs on the local machine, on a Spark cluster and on a Dask cluster. Please note that only one of the three following code cells should be executed, depending on what backend we aim for. **The rest of the analysis is exactly the same** no matter what choice we made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local\n",
    "\n",
    "The full dataset contains half a year of CMS data taking in 2012 with 61 mio events. For the purpose of this example, we use the `Range` node to run only on a small part of the dataset when we only exploit the local cores of our SWAN session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a local RDataFrame\n",
    "RDataFrame = ROOT.RDataFrame\n",
    "\n",
    "\n",
    "# Create the RDataFrame\n",
    "df = RDataFrame(\"Events\",\n",
    "                \"root://eospublic.cern.ch//eos/opendata/cms/derived-data/AOD2NanoAODOutreachTool/Run2012BC_DoubleMuParked_Muons.root\")\n",
    "\n",
    "# Take only the first 1M events\n",
    "df = df.Range(1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark\n",
    "\n",
    "In order to work with a Spark cluster we need a `SparkContext` object, which represents the connection to that cluster and allows to configure execution-related parameters (e.g. number of cores, memory). When running this notebook from [SWAN](https://swan.cern.ch), a `SparkContext` object is already created for us when connecting to the selected cluster via the graphical interface. Alternatively, we could create a `SparkContext` as described in the [Spark documentation](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkContext.html).\n",
    "\n",
    "The constructor of a Spark `RDataFrame` receives one extra parameter: the `SparkContext` object (`sparkcontext`). Besides that detail, a Spark `RDataFrame` is not different from a local `RDataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark context is provided by SWAN, but it could be created manually\n",
    "sc.addPyFile(\"./DistRDF.zip\")\n",
    "\n",
    "# Use a Spark RDataFrame\n",
    "RDataFrame = ROOT.RDF.Experimental.Distributed.Spark.RDataFrame\n",
    "\n",
    "# Create the RDataFrame\n",
    "df = RDataFrame(\"Events\",\n",
    "                \"root://eospublic.cern.ch//eos/opendata/cms/derived-data/AOD2NanoAODOutreachTool/Run2012BC_DoubleMuParked_Muons.root\",\n",
    "                sparkcontext=sc)\n",
    "\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask\n",
    "\n",
    "In order to work with a Dask cluster we first create a Dask `Client` object, which allows us to connect to a cluster we deployed for this example.\n",
    "\n",
    "The constructor of a Dask `RDataFrame` receives one extra parameter: the Dask `Client` object (`daskclient`). Besides that detail, a Dask `RDataFrame` is not different from a local `RDataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Dask client\n",
    "from dask.distributed import Client\n",
    "client = Client(\"tcp://distrdf-master.cern.ch:8786\")\n",
    "\n",
    "# Use a Dask RDataFrame\n",
    "RDataFrame = ROOT.RDF.Experimental.Distributed.Dask.RDataFrame\n",
    "\n",
    "# Create the RDataFrame\n",
    "df = RDataFrame(\"Events\",\n",
    "                \"root://eospublic.cern.ch//eos/opendata/cms/derived-data/AOD2NanoAODOutreachTool/Run2012BC_DoubleMuParked_Muons.root\",\n",
    "                daskclient=client)\n",
    "\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter relevant events for this analysis\n",
    "\n",
    "Physics datasets are often general purpose datasets and therefore need extensive filtering of the events for the actual analysis. Here, we implement only a simple selection based on the number of muons and the charge to cut down the dataset in events that are relevant for our study.\n",
    "\n",
    "In particular, we are applying two filters to keep:\n",
    "1. Events with exactly two muons\n",
    "2. Events with muons of opposite charge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2mu = df.Filter(\"nMuon == 2\", \"Events with exactly two muons\")\n",
    "df_oc = df_2mu.Filter(\"Muon_charge[0] != Muon_charge[1]\", \"Muons with opposite charge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the invariant mass of the dimuon system\n",
    "\n",
    "Since we want to see the resonances in the mass spectrum, where dimuon events are more likely, we need to compute the invariant mass from the four-vectors of the muon candidates. Because this operation is non-trivial, we will use ROOT's `ROOT::VecOps::InvariantMass` function to do the job for us.\n",
    "\n",
    "The invariant mass will be stored in a new column that we will create with the `Define` operation of `RDataFrame`. The parameters of `Define` are the name of the new column and a string with the function to be invoked, which includes the dataset columns to be used as arguments for the such function.\n",
    "\n",
    "Note how, even though `ROOT::VecOps::InvariantMass` is a C++ function, we can use it in our `RDataFrame` workflow from Python: the second parameter of `Define` is a string that contains C++ code. Such code will be just-in-time compiled by ROOT and called during the event loop in C++. This allows to benefit from C++ performance in computationally-expensive code even when programming in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mass = df_oc.Define(\"Dimuon_mass\", \"ROOT::VecOps::InvariantMass(Muon_pt, Muon_eta, Muon_phi, Muon_mass)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a histogram of the dimuon spectrum\n",
    "\n",
    "As (almost) always in physics, we have a look at the results in the form of a histogram. Let's book a histogram as one endpoint of our computation graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbins = 30000\n",
    "low = 0.25\n",
    "up = 300\n",
    "h = df_mass.Histo1D((\"Dimuon_mass\", \"Dimuon_mass\", nbins, low, up), \"Dimuon_mass\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the dimuon spectrum\n",
    "\n",
    "Now, the computation graph is set up. Next, we want to have a look at the result.\n",
    "\n",
    "Note that the event loop actually runs the first time we try to access the histogram object in the next cell (results of an `RDataFrame` graph are computed lazily). The kind of parallelization that will be triggered here depends on the choice we made earlier in this notebook (multi-threaded on local cores, distributed with Spark or distributed with Dask).\n",
    "\n",
    "`%%time` measures the time spent in the full cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ROOT.gStyle.SetOptStat(0); ROOT.gStyle.SetTextFont(42)\n",
    "c = ROOT.TCanvas(\"c\", \"\", 800, 700)\n",
    "c.SetLogx(); c.SetLogy()\n",
    "h.SetTitle(\"\")\n",
    "h.GetXaxis().SetTitle(\"m_{#mu#mu} (GeV)\"); h.GetXaxis().SetTitleSize(0.04)\n",
    "h.GetYaxis().SetTitle(\"N_{Events}\"); h.GetYaxis().SetTitleSize(0.04)\n",
    "h.Draw()\n",
    "\n",
    "label = ROOT.TLatex(); label.SetNDC(True)\n",
    "label.SetTextSize(0.040); label.DrawLatex(0.100, 0.920, \"#bf{CMS Open Data}\")\n",
    "label.SetTextSize(0.030); label.DrawLatex(0.500, 0.920, \"#sqrt{s} = 8 TeV, L_{int} = 11.6 fb^{-1}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROOT provides interactive JavaScript graphics for Jupyter, which can be activated with the `%jsroot` magic. Click and drag on the axis to zoom in and double click to reset the view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%jsroot on\n",
    "c.Draw()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "sparkconnect": {
   "bundled_options": [],
   "list_of_options": [
    {
     "name": "spark.dynamicAllocation.enabled",
     "value": "false"
    },
    {
     "name": "spark.executor.cores",
     "value": "4"
    },
    {
     "name": "spark.executor.instances",
     "value": "4"
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
